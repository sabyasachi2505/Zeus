{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(\n",
    "  [\n",
    "    [1, 'alpha', '1'],\n",
    "    [2, 'beta', '0'],\n",
    "    [3, 'gamma', '1']\n",
    "  ],\n",
    "  ['age', 'name', 'target']\n",
    ")\n",
    "from zeus import Zeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alpha'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(e for e in 'alpha/' if e.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string, fields: array<struct<name:string,nullable:boolean,type:string>>, type: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\") \\\n",
    ".option(\"inferSchema\", \"true\").option(\"header\", \"true\").load('gs://ds-data-model-build-poc-tier1/unified_schema/cross_retailer_dataset_20180412/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(apn_user_id=u'1000017848450513856', ts=u'2016-11-24 19:24:03.268', tdm_user_id=u'be76a49b-1ae8-4401-9877-b89b32bd2ac1', opt_out=u'0', funnel_stage=u'1', department=None, category=None, sub_category=None, brand=None, product=None, ip_address=u'172.75.242.222', session_id=u'1480015443268', year=u'2016', month=u'11', day=u'24', tdm_retailer_id=u'23573', party_id=u'68573748-2218-3e20-a78a-a66e92de1687', date1=datetime.date(2016, 11, 24), liveramp_id=u'XY6441GhhqKfOg-NejsyCpwHXFh9yG7SOQFxXG34XvqA5hlGU'),\n",
       " Row(apn_user_id=u'1000017848450513856', ts=u'2017-12-14 16:42:08.616', tdm_user_id=u'be76a49b-1ae8-4401-9877-b89b32bd2ac1', opt_out=u'0', funnel_stage=u'1', department=None, category=None, sub_category=None, brand=None, product=None, ip_address=u'66.201.46.201', session_id=u'1513269728616', year=u'2017', month=u'12', day=u'14', tdm_retailer_id=u'85989', party_id=u'68573748-2218-3e20-a78a-a66e92de1687', date1=datetime.date(2017, 12, 14), liveramp_id=u'XY6441GhhqKfOg-NejsyCpwHXFh9yG7SOQFxXG34XvqA5hlGU'),\n",
       " Row(apn_user_id=u'1000017848450513856', ts=u'2016-11-24 19:24:13.199', tdm_user_id=u'be76a49b-1ae8-4401-9877-b89b32bd2ac1', opt_out=u'0', funnel_stage=u'1', department=None, category=None, sub_category=None, brand=None, product=None, ip_address=u'172.75.242.222', session_id=u'1480015443268', year=u'2016', month=u'11', day=u'24', tdm_retailer_id=u'23573', party_id=u'68573748-2218-3e20-a78a-a66e92de1687', date1=datetime.date(2016, 11, 24), liveramp_id=u'XY6441GhhqKfOg-NejsyCpwHXFh9yG7SOQFxXG34XvqA5hlGU'),\n",
       " Row(apn_user_id=u'1000017848450513856', ts=u'2017-05-03 04:16:51.971', tdm_user_id=u'be76a49b-1ae8-4401-9877-b89b32bd2ac1', opt_out=u'0', funnel_stage=u'3', department=u'search', category=u'search:Purple lipstick', sub_category=u'search>purple lipstick', brand=None, product=None, ip_address=u'166.216.157.117', session_id=u'1493784901261', year=u'2017', month=u'5', day=u'3', tdm_retailer_id=u'11629', party_id=u'68573748-2218-3e20-a78a-a66e92de1687', date1=datetime.date(2017, 5, 3), liveramp_id=u'XY6441GhhqKfOg-NejsyCpwHXFh9yG7SOQFxXG34XvqA5hlGU'),\n",
       " Row(apn_user_id=u'1000017848450513856', ts=u'2016-11-26 20:18:00.858', tdm_user_id=u'be76a49b-1ae8-4401-9877-b89b32bd2ac1', opt_out=u'0', funnel_stage=u'3', department=u'cat980731', category=u'cat61880803', sub_category=None, brand=None, product=None, ip_address=u'107.77.104.17', session_id=u'1480191480858', year=u'2016', month=u'11', day=u'26', tdm_retailer_id=u'85989', party_id=u'68573748-2218-3e20-a78a-a66e92de1687', date1=datetime.date(2016, 11, 26), liveramp_id=u'XY6441GhhqKfOg-NejsyCpwHXFh9yG7SOQFxXG34XvqA5hlGU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"parquet\") \\\n",
    ".option(\"inferSchema\", \"true\").option(\"header\", \"true\").load('gs://ds-mlengine/shopper_profile/site_visits_regular_0504_2018').head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_zs = Zeus(data, targetColumn = 'target', idColumn = 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-ed7e2c236c13>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-ed7e2c236c13>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    avro :\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_zs.columns()\n",
    "ftype = {\n",
    "  'avro' : \"com.databricks.spark.avro\",\n",
    "  'parquet' : \"parquet\",\n",
    "  'csv' : \"com.databricks.spark.csv\",\n",
    "  'json' : \"json\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1266803239c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "{}.insert((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Failed to find data source: avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-84c80d74d834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://ds-mlengine/sabyasachi/testing_zeus1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Failed to find data source: avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;'"
     ]
    }
   ],
   "source": [
    "data.write.format(\"com.databricks.spark.avro\").save('gs://ds-mlengine/sabyasachi/testing_zeus1', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|age| name|target|\n",
      "+---+-----+------+\n",
      "|  1|alpha|     1|\n",
      "|  3|gamma|     1|\n",
      "|  2| beta|     0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.avro('gs://ds-mlengine/sabyasachi/testing_zeus').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
